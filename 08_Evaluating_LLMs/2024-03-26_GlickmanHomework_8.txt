I think that idea generation is an excellent use case of LLMs and generative AI models in general for science. We have seen the utility of LLMs in proposing candidate proteins and materials for experimentation. LLMs can generate huge amounts of these candidates well beyond what a human scientist can. These models can be trained to generate proteins and materials that have a high degree of promise when engineered and tested for desirable qualities. Using models in this way, scientists can cheaply and efficiently identify the most promising candidates in silico before investing resources into laboratory or field experiments. This will increase the productivity of scientists and stretch the already limited funding for scientific research well beyond what was possible before.

This use case may be evaluate in a simple manner. Suppose we seek bidegradable materials to replace petroleum based plastics. Our LLM trained on lots of examples of materials and their physical properties, generates us candidate materials which may meet our goals. Suppose we generate 10,000 materials and consider the top 100 for in silico experimentation. From there, we observe 10 that meet our desired qualities. From there, we engineer these top 10 materials and design laboratory experiments to test their qualities. We find 2 that meet our criteria. We can calculate the yield of our model at 3 levels: initial promise (pre silico experimentation), in silico success, andlaboratory success relative to the total materials generated. Additionally, we could look at ratio of success at each of these 3 levels. Beyond that, we could represent the materials in some vector space and look at distances from successful candidates to asses possible regions of success in the embedding space.

These types of experiments are no longer science fiction. AI and HPC will be two of the main drivers of scientific innovation for the forseeable furture, and scientists must ensure that LLMs are employed effectively and responsibly.
